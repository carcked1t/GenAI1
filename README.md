# GenAI Project- PDF Chat App with Optional General Knowledge Toggle

# ğŸ“„ Chat with PDFs using Gemini (LangChain + Streamlit)

An interactive web application that allows users to **upload PDF documents and chat with them intelligently** using Googleâ€™s Gemini models.  
The app supports **document-grounded answers with source citations** as well as an optional **mixed mode** where the model can combine document context with its own general knowledge.


## ğŸŒ Live Demo

ğŸ”— **Deployed App**:  
ğŸ‘‰ *(https://genai1-chat-pdf.streamlit.app/)*



## âœ¨ Features

- ğŸ“‚ Upload multiple PDF files
- ğŸ” Semantic search over document content using **FAISS**
- ğŸ¤– Question answering powered by **Google Gemini**
- ğŸ“Œ **Source citations** showing which document chunks were used
- ğŸ”€ Two answer modes:
  - **Document-only mode** (strictly answers from PDFs)
  - **Hybrid mode** (PDF + general knowledge)
- âš¡ Optimized performance using Streamlit caching
- ğŸ” Secure API key handling via environment variables

---

## ğŸ›  Tech Stack

- **Frontend / UI**: Streamlit
- **LLM**: Google Gemini (via `langchain-google-genai`)
- **Embeddings**: HuggingFace (`all-MiniLM-L6-v2`)
- **Vector Store**: FAISS
- **PDF Parsing**: PyPDF2
- **Framework**: LangChain (modular v0.1+ architecture)

---

## ğŸš€ How It Works (High-Level)

1. User uploads one or more PDFs
2. PDFs are:
   - Extracted page-by-page
   - Split into overlapping semantic chunks
3. Chunks are embedded and stored locally using FAISS
4. On each question:
   - Relevant chunks are retrieved via similarity search
   - A structured prompt is created
   - Gemini generates an answer
   - Source chunks are returned as citations

## ğŸ§ª Answer Modes Explained

### ğŸ“˜ Document-Only Mode
- The model is **restricted** to the retrieved PDF context
- If the answer is not found, it explicitly says so
- Ideal for:
  - Academic material
  - Legal / policy documents
  - Notes and reports

### ğŸŒ Hybrid Mode
- The model can:
  - Use document context **first**
  - Supplement with general knowledge if needed
- Ideal for:
  - Learning
  - Research
  - Concept clarification


## ğŸ§‘â€ğŸ’» Running Locally

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/carcked1t/GenAI1.git
cd GenAI1
````

### 2ï¸âƒ£ Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Add environment variables

Create a `.env` file:

```env
GOOGLE_API_KEY=your_api_key_here
```

### 5ï¸âƒ£ Run the app

```bash
streamlit run app.py
```

---

## ğŸ“ Project Structure

```
GenAI1/
â”‚
â”œâ”€â”€ app.py                 # Streamlit application
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ faiss_index/            # Vector store (generated locally)
â”œâ”€â”€ venv/                   # Virtual environment (ignored)
```

---

## âš ï¸ Challenges Faced & Solutions

### 1ï¸âƒ£ LangChain Import Errors

**Issue:**
Frequent `ModuleNotFoundError` due to breaking changes in LangChain.

**Solution:**
Migrated to the new modular structure:

* `langchain_core`
* `langchain_community`
* `langchain_text_splitters`

---

### 2ï¸âƒ£ Gemini Model Availability Errors

**Issue:**
Some Gemini model names returned `404 NOT_FOUND`.

**Solution:**
Used `models/gemini-1.5-flash` / `models/gemini-flash-latest`
Verified model availability via Googleâ€™s API documentation.

---

### 3ï¸âƒ£ Slow Streamlit Reloads

**Issue:**
Every interaction reloaded embeddings and models.

**Solution:**
Implemented:

```python
@st.cache_resource
```

to cache:

* Embeddings
* FAISS index
* LLM chain

---

### 4ï¸âƒ£ â€œAnswer Not Foundâ€ for All Queries

**Issue:**
Prompt was too restrictive and context retrieval was insufficient.

**Solution:**

* Improved chunk size & overlap
* Retrieved top-K relevant chunks
* Added hybrid answer mode

---

### 5ï¸âƒ£ API Quota Exhaustion

**Issue:**
Free Gemini quota exhausted during testing.

**Solution:**

* Reduced unnecessary calls
* Cached chains
* Planned fallback handling for deployment

---

## How This Is Different from Typical â€œChat with PDFâ€ Apps

* âœ… **Clear separation between document-only and hybrid reasoning**
* âœ… **Explicit source citations**, not just answers
* âœ… Built with **modern LangChain architecture**
* âœ… Optimized for real deployment, not just demos
* âœ… Transparent failure handling (e.g., â€œanswer not foundâ€)

Rather than aiming to be flashy, this project focuses on **reliability, clarity, and real-world usability**.

---

## ğŸ“Œ Future Improvements
* Conversation memory
* Streaming responses
* Support for DOCX / TXT files
* Model fallback when Gemini quota is exceeded
